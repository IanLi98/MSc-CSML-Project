#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Aug 31 10:11:27 2021

@author: Ian_20063972
"""
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.svm import SVC, LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import Ridge, ElasticNet, Lasso

# ridge regression
def ridge_regression(X, y, reg_param):
    model = Ridge(alpha=reg_param, fit_intercept=False)
    model.fit(X, y)
    return model.coef_

# lasso
def lasso(X, y, reg_param):
    model = Lasso(alpha=reg_param, fit_intercept=False)
    model.fit(X, y)
    return model.coef_

# elastic net
def elastic_net(X, y, reg_param, l1_param):
    model = ElasticNet(alpha=reg_param, l1_ratio=l1_param, fit_intercept=False)
    model.fit(X, y)

# exclusive group Lasso with re-weighted scheme
class ReWeightedEGL():
    
    def __init__(self, reg_param, n_group, thr, n_iter, idx_group_mat, idx_group_list, feat_dict):
        self.reg_param = reg_param
        self.idx_group_mat = idx_group_mat  
        # matrix, size: n_group*n_feature, where entries are 1 (group contains the feature) 
        # or 0 (group does not contain the feature)
        self.idx_group_list = idx_group_list  
        # list of list. length: n_group, where entry i is the list of features in group i.
        self.feat_dict = feat_dict  
        # dictionary. key: feature, value: group containing the feature
        self.n_group = n_group
        self.thr = thr
        self.n_iter = n_iter
        self.coef = None
        self.idx = None
        #self.converged = False

    def _compute_Finv(self, w):
        
        n_group = self.idx_group_mat.shape[0]
        n_feature = self.idx_group_mat.shape[1]
        # initialization of ||w||_{1}
        w_group_norm = np.empty(n_group)
        for i in range(n_group): 
            w_group = w[self.idx_group_list[i]]
            w_group_norm[i] = np.linalg.norm(w_group, ord=1)
        w_group_norm[np.where(w_group_norm==0)[0]] = 10**-9 #in case the denominator is zero
        w_abs = np.abs(w)
        F_inv_diag = np.zeros(n_feature)
        for j in range(n_feature):
            F_inv_diag[j] = w_abs[j] / w_group_norm[self.feat_dict[j]]
        G_diag = np.sqrt(F_inv_diag)
        
        return G_diag  # (F_{jj})^{-\frac{1}{2}}    

    def _compute_X_tran(self, X, G_diag): # compute X'
        
        return X.dot(np.diag(G_diag)) # X'=X(F)^{-\frac{1}{2}}
    
    def _compute_w_tran(self, X_tran, y): # compute w'
        
        if len(np.unique(y)) == 2:
            clf = LinearSVC(C=self.reg_param, fit_intercept=False)
            clf.fit(X_tran, y)
            w = clf.coef_
        else:
            clf = Ridge(alpha=self.reg_param, fit_intercept=False)
            clf.fit(X_tran, y)
            w = clf.coef_
        
        return w

    def _create_rand_group(self, n_feature): # generate random group allocation
        
        self.idx_group_mat = np.zeros((self.n_group, n_feature))
        idx = np.random.permutation(n_feature)
        idx = np.array_split(idx, self.n_group)
        for i, j in enumerate(idx):
            self.idx_group_mat[i, j] = 1    
    
    def _ell12_norm(self, X, y):
        
        n_sample, n_feature = X.shape

        if self.idx_group_mat is None and self.idx_group_list is None and self.feat_dict is None:
            self._create_rand_group(n_feature)

        if self.feat_dict is None:
            if self.idx_group_list is None:
                self.idx_group_list = []
                n_group = self.idx_group_mat.shape[0]
            else:
                n_group = len(self.idx_group_list)
            feat_dict = {}
            for i in range(n_group):
                if not self.idx_group_list:
                    temp = np.nonzero(self.idx_group_mat[i, :])[0]
                    self.idx_group_list.append(temp)
                else:
                    temp = self.idx_group_list[i]
                for j in temp:
                    feat_dict[j] = i
            self.feat_dict = feat_dict

        w = np.ones(n_feature) / n_feature #initlization of uniform w

        G_diag = self._compute_Finv(w)
        X_tran = self._compute_X_tran(X, G_diag)
        w_tran = self._compute_w_tran(X_tran, y)

        t = 0 # iteration counter
        while True:
            t += 1
            w_pre = w.copy()
            w = w_tran*G_diag

            G_diag = self._compute_G(w)
            X_tran = self._compute_X_tran(X, G_diag)
            w_tran = self._compute_w_tran(X_tran, y)

            tol = np.linalg.norm(w_pre-w)

            if tol <= self.thr or t >= self.n_iter:
                break

        self.coef = w
        self.idx = np.where(np.abs(w)>10**-3)[0]

        if t < self.n_iter:
            print('Converged!')

    def fit(self, X, y):
        self._ell12_norm(X, y)
 

# import data
# URL of data: https://web.stanford.edu/~hastie/CASI_files/DATA/diabetes.html
df = pd.read_csv('diabetes.csv', dtype=np.float32)
data = df.values
X = data[:,1:-1]
Y = data[:,-1]

# experiments of lasso for different control parameters
reg_param_list = np.linspace(1,20000,100000)
lasso_coef = np.zeros((100000,10))
for i in range(100000):
    lasso_coef[i,:] = lasso(X,Y,reg_param_list[i])

#plt.plot(np.log(reg_param_list), lasso_coef[:,0])  #abandon
#plt.plot(np.log(reg_param_list), lasso_coef[:,1])  #abandon
plt.plot(np.log(reg_param_list), lasso_coef[:,2], label='bmi')      
plt.plot(np.log(reg_param_list), lasso_coef[:,3], label='map')  
plt.plot(np.log(reg_param_list), lasso_coef[:,4], label='tc') 
plt.plot(np.log(reg_param_list), lasso_coef[:,5], label='ldl')
plt.plot(np.log(reg_param_list), lasso_coef[:,6], label='hdl')
plt.plot(np.log(reg_param_list), lasso_coef[:,7], label='tch') 
#plt.plot(np.log(reg_param_list), lasso_coef[:,8]) #abandon
#plt.plot(np.log(reg_param_list), lasso_coef[:,9]) #abandon
plt.plot(np.log(reg_param_list), np.zeros(100000), color='black')
plt.xlabel('log(reg_param)')
plt.ylabel('weight')
plt.legend()
plt.title('Lasso')
# experiments of ridge regression for different control parameters
# more parameters here to make the diffenernce more significant
reg_param_list_l = np.linspace(1,200000,500000)
ridge_coef = np.zeros((500000,10))
for i in range(500000):
    ridge_coef[i,:] = ridge_regression(X,Y,reg_param_list_l[i])

plt.plot(np.log(reg_param_list_l), ridge_coef[:,2], label='bmi')      
plt.plot(np.log(reg_param_list_l), ridge_coef[:,3], label='map')   
plt.plot(np.log(reg_param_list_l), ridge_coef[:,4], label='tc')  
plt.plot(np.log(reg_param_list_l), ridge_coef[:,5], label='ldl')
plt.plot(np.log(reg_param_list_l), ridge_coef[:,6], label='hdl')
plt.plot(np.log(reg_param_list_l), ridge_coef[:,7], label='tch') 
plt.plot(np.log(reg_param_list_l), np.zeros(500000), color='black')
plt.ylim(ymax=9)
plt.xlabel('log(reg_param)')
plt.ylabel('weight')
plt.legend()
plt.title('Ridge regression')
    

# the following code is used for demonstrate the dfference between lasso and elastic net
# artificial data
z_1 = np.random.uniform(0,30,100)
z_2 = np.random.uniform(0,30,100)
y = 3*z_1 + 0.5*z_2 + np.random.normal(0,1,100)
x_1 = z_1 + np.random.normal(0,1,100)
x_2 = z_1 + np.random.normal(0,1,100)
x_3 = -z_1 + np.random.normal(0,1,100)
x_4 = z_2 + np.random.normal(0,1,100)
x_5 = -z_2 + np.random.normal(0,1,100)
X = np.vstack((x_1,x_2,x_3,x_4,x_5)).T
reg_param = np.linspace(0,1000,10001)
coef_mat = np.zeros((10001,5))

for i in range(10001):
    EN = ElasticNet(alpha=reg_param[i], l1_ratio=0.5)
    EN.fit(X,y)
    coef_mat[i,:] = EN.coef_
plt.plot(reg_param,coef_mat[:,0],label='x_1')
plt.plot(reg_param,coef_mat[:,1],label='x_2')
plt.plot(reg_param,coef_mat[:,2],label='x_3')
plt.plot(reg_param,coef_mat[:,3],label='x_4')
plt.plot(reg_param,coef_mat[:,4],label='x_5')
plt.xlabel('lambda')
plt.ylabel('weights')
plt.title('Elastic net')
plt.legend(loc='upper right')

for i in range(10001):
    LASSO = Lasso(alpha=reg_param[i])
    LASSO.fit(X,y)
    coef_mat[i,:] = LASSO.coef_
plt.plot(reg_param,coef_mat[:,0],label='x_1')
plt.plot(reg_param,coef_mat[:,1],label='x_2')
plt.plot(reg_param,coef_mat[:,2],label='x_3')
plt.plot(reg_param,coef_mat[:,3],label='x_4')
plt.plot(reg_param,coef_mat[:,4],label='x_5')
plt.xlabel('lambda')
plt.ylabel('weights')
plt.title('Lasso')
plt.legend(loc='upper right')
    
    
    
    
    
    
    
    
    
    
