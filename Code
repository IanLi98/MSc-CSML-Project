#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Aug 31 10:11:27 2021

@author: 20063972
"""
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.svm import SVC, LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate
from sklearn.metrics import confusion_matrix
from sklearn.linear_model import Ridge, ElasticNet, Lasso

# ridge regression
def ridge_regression(X, y, reg_param):
    model = Ridge(alpha=reg_param, fit_intercept=False)
    model.fit(X, y)
    return model.coef_

# lasso
def lasso(X, y, reg_param):
    model = Lasso(alpha=reg_param, fit_intercept=False)
    model.fit(X, y)
    return model.coef_

# elastic net
def elastic_net(X, y, reg_param, l1_param):
    model = ElasticNet(alpha=reg_param, l1_ratio=l1_param, fit_intercept=False)
    model.fit(X, y)

# exclusive group Lasso with re-weighted scheme
class ReWeightedEGL():
    
    def __init__(self, reg_param, n_group, thr, n_iter, idx_group_mat, idx_group_list, feat_dict):
        self.reg_param = reg_param
        self.idx_group_mat = idx_group_mat  
        # matrix, size: n_group*n_feature, where entries are 1 (group contains the feature) 
        # or 0 (group does not contain the feature)
        self.idx_group_list = idx_group_list  
        # list of list. length: n_group, where entry i is the list of features in group i.
        self.feat_dict = feat_dict  
        # dictionary. key: feature, value: group containing the feature
        self.n_group = n_group
        self.thr = thr
        self.n_iter = n_iter
        self.coef = None
        self.idx = None
        #self.converged = False

    def _compute_Finv(self, w):
        
        n_group = self.idx_group_mat.shape[0]
        n_feature = self.idx_group_mat.shape[1]
        # initialization of ||w||_{1}
        w_group_norm = np.empty(n_group)
        for i in range(n_group): 
            w_group = w[self.idx_group_list[i]]
            w_group_norm[i] = np.linalg.norm(w_group, ord=1)
        w_group_norm[np.where(w_group_norm==0)[0]] = 10**-9 #in case the denominator is zero
        w_abs = np.abs(w)
        F_inv_diag = np.zeros(n_feature)
        for j in range(n_feature):
            F_inv_diag[j] = w_abs[j] / w_group_norm[self.feat_dict[j]]
        G_diag = np.sqrt(F_inv_diag)
        
        return G_diag  # (F_{jj})^{-\frac{1}{2}}    

    def _compute_X_tran(self, X, G_diag): # compute X'
        
        return X.dot(np.diag(G_diag)) # X'=X(F)^{-\frac{1}{2}}
    
    def _compute_w_tran(self, X_tran, y): # compute w'
        
        if len(np.unique(y)) == 2:
            clf = LinearSVC(C=self.reg_param, fit_intercept=False)
            clf.fit(X_tran, y)
            w = clf.coef_
        else:
            clf = Ridge(alpha=self.reg_param, fit_intercept=False)
            clf.fit(X_tran, y)
            w = clf.coef_
        
        return w

    def _create_rand_group(self, n_feature): # generate random group allocation
        
        self.idx_group_mat = np.zeros((self.n_group, n_feature))
        idx = np.random.permutation(n_feature)
        idx = np.array_split(idx, self.n_group)
        for i, j in enumerate(idx):
            self.idx_group_mat[i, j] = 1    
    
    def _ell12_norm(self, X, y):
        
        n_sample, n_feature = X.shape

        if self.idx_group_mat is None and self.idx_group_list is None and self.feat_dict is None:
            self._create_rand_group(n_feature)

        if self.feat_dict is None:
            if self.idx_group_list is None:
                self.idx_group_list = []
                n_group = self.idx_group_mat.shape[0]
            else:
                n_group = len(self.idx_group_list)
            feat_dict = {}
            for i in range(n_group):
                if not self.idx_group_list:
                    temp = np.nonzero(self.idx_group_mat[i, :])[0]
                    self.idx_group_list.append(temp)
                else:
                    temp = self.idx_group_list[i]
                for j in temp:
                    feat_dict[j] = i
            self.feat_dict = feat_dict

        w = np.ones(n_feature) / n_feature #initlization of uniform w

        G_diag = self._compute_Finv(w)
        X_tran = self._compute_X_tran(X, G_diag)
        w_tran = self._compute_w_tran(X_tran, y)

        t = 0 # iteration counter
        while True:
            t += 1
            w_pre = w.copy()
            w = w_tran*G_diag

            G_diag = self._compute_G(w)
            X_tran = self._compute_X_tran(X, G_diag)
            w_tran = self._compute_w_tran(X_tran, y)

            tol = np.linalg.norm(w_pre-w)

            if tol <= self.thr or t >= self.n_iter:
                break

        self.coef = w
        self.idx = np.where(np.abs(w)>10**-3)[0]

        if t < self.n_iter:
            print('Converged!')

    def fit(self, X, y):
        self._ell12_norm(X, y)
 
class stab_select():
    def __init__(self, param_range=np.linspace(0.01, 1, 100), alpha=0.5, sample_size=None, p_threshold=0.5, n_iter=100,
                 reg_type='lasso', verbose=0, idx_group=None, n_group=50, group_type='fixed'):
        self.param_range = param_range
        self.alpha = alpha
        self.sample_size = sample_size
        self.p_threshold = p_threshold
        self.n_iter = n_iter
        self.reg_type = reg_type
        self.verbose = verbose
        self.idx_group = idx_group
        self.n_group = n_group
        self.group_type = 'fixed'

    def _compute_w(self, X, y):
        return fitLasso(X, y, param_range=self.param_range, alpha=self.alpha, reg_type=self.reg_type,
                        idx_group=self.idx_group, n_group=self.n_group, group_type=self.group_type)

    def _select_feature(self, w_vec):
        # w_vec_new = np.zeros(w_vec.shape)
        # w_vec_new[np.where(abs(w_vec) >= 10**-3)] = 1.

        w_vec_new = sp.lil_matrix(w_vec.shape)
        w_vec_new[np.where(abs(w_vec) >= 10 ** -3)] = 1.

        return sp.csr_matrix(w_vec_new)

    def _get_weights(self, w_vec):
        idx = np.where(abs(w_vec) >= 10 ** -3)
        w = w_vec[idx]

        return w, idx[0]

    def _fit(self, X, y):
        n_sample, n_feature = X.shape
        if self.sample_size is None:
            self.sample_size = int(n_sample / 2)

        # w_counter = np.zeros((n_feature, len(self.param_range)))
        w_counter = sp.csr_matrix((n_feature, len(self.param_range)))
        w_vals = []
        idx_vals = []
        convergence = []
        for counter in range(self.n_iter):

            if self.verbose == 1:
                print('iteration: %d/%d.' % (counter, self.n_iter))

            # subsample samples
            idx_rand = np.random.permutation(n_sample)
            sub_idx = idx_rand[:self.sample_size]

            X_sub = X[sub_idx, :]
            y_sub = y[sub_idx]

            # fit subsamples to get coefficients
            w_vec, converged = self._compute_w(X_sub, y_sub)
            if converged:
                w_counter += self._select_feature(w_vec)

            w_temp, idx_temp = self._get_weights(w_vec)
            # w_vals.append(w_temp)
            w_vals.append(w_vec)
            idx_vals.append(idx_temp)
            convergence.append(converged)

            # [added/modified] repeat for the other half subsamples
            if self.sample_size <= int(n_sample / 2):
                sub_idx = idx_rand[self.sample_size:]

                X_sub = X[sub_idx, :]
                y_sub = y[sub_idx]

                # fit subsamples to get coefficients
                w_vec, converged = self._compute_w(X_sub, y_sub)
                if converged:
                    w_counter += self._select_feature(w_vec)

                w_temp, idx_temp = self._get_weights(w_vec)
                # w_vals.append(w_temp)
                w_vals.append(w_vec)
                idx_vals.append(idx_temp)
                convergence.append(converged)

        # select features with selection probabilities larger than p_threshold, under all lambdas
        select_prob = w_counter / float(len(np.where(convergence)[0]))
        max_select_prob = np.max(select_prob, axis=1).toarray()
        idx = np.where(max_select_prob >= self.p_threshold)[0]

        self.idx = idx
        self.w_count = w_counter
        self.select_prob = select_prob
        self.max_prob = max_select_prob
        self.weights_total = w_vals
        self.idx_total = idx_vals
        self.convergence = convergence

    def fit(self, X, y):
        self._fit(X, y)

    def predict(self, X):
        return np.ravel(np.dot(X[:, self.idx], self.coef))

# import data
# URL of data: https://web.stanford.edu/~hastie/CASI_files/DATA/diabetes.html
df = pd.read_csv('diabetes.csv', dtype=np.float32)
data = df.values
X = data[:,1:-1]
Y = data[:,-1]

# experiments of lasso for different control parameters
reg_param_list = np.linspace(1,20000,100000)
lasso_coef = np.zeros((100000,10))
for i in range(100000):
    lasso_coef[i,:] = lasso(X,Y,reg_param_list[i])

#plt.plot(np.log(reg_param_list), lasso_coef[:,0])  #abandon
#plt.plot(np.log(reg_param_list), lasso_coef[:,1])  #abandon
plt.plot(np.log(reg_param_list), lasso_coef[:,2], label='bmi')      
plt.plot(np.log(reg_param_list), lasso_coef[:,3], label='map')  
plt.plot(np.log(reg_param_list), lasso_coef[:,4], label='tc') 
plt.plot(np.log(reg_param_list), lasso_coef[:,5], label='ldl')
plt.plot(np.log(reg_param_list), lasso_coef[:,6], label='hdl')
plt.plot(np.log(reg_param_list), lasso_coef[:,7], label='tch') 
#plt.plot(np.log(reg_param_list), lasso_coef[:,8]) #abandon
#plt.plot(np.log(reg_param_list), lasso_coef[:,9]) #abandon
plt.plot(np.log(reg_param_list), np.zeros(100000), color='black')
plt.xlabel('log(reg_param)')
plt.ylabel('weight')
plt.legend()
plt.title('Lasso')
# experiments of ridge regression for different control parameters
# more parameters here to make the diffenernce more significant
reg_param_list_l = np.linspace(1,200000,500000)
ridge_coef = np.zeros((500000,10))
for i in range(500000):
    ridge_coef[i,:] = ridge_regression(X,Y,reg_param_list_l[i])

plt.plot(np.log(reg_param_list_l), ridge_coef[:,2], label='bmi')      
plt.plot(np.log(reg_param_list_l), ridge_coef[:,3], label='map')   
plt.plot(np.log(reg_param_list_l), ridge_coef[:,4], label='tc')  
plt.plot(np.log(reg_param_list_l), ridge_coef[:,5], label='ldl')
plt.plot(np.log(reg_param_list_l), ridge_coef[:,6], label='hdl')
plt.plot(np.log(reg_param_list_l), ridge_coef[:,7], label='tch') 
plt.plot(np.log(reg_param_list_l), np.zeros(500000), color='black')
plt.ylim(ymax=9)
plt.xlabel('log(reg_param)')
plt.ylabel('weight')
plt.legend()
plt.title('Ridge regression')
    

# the following code is used for demonstrate the dfference between lasso and elastic net
# artificial data
z_1 = np.random.uniform(0,30,100)
z_2 = np.random.uniform(0,30,100)
y = 3*z_1 + 0.5*z_2 + np.random.normal(0,1,100)
x_1 = z_1 + np.random.normal(0,1,100)
x_2 = z_1 + np.random.normal(0,1,100)
x_3 = -z_1 + np.random.normal(0,1,100)
x_4 = z_2 + np.random.normal(0,1,100)
x_5 = -z_2 + np.random.normal(0,1,100)
X = np.vstack((x_1,x_2,x_3,x_4,x_5)).T
reg_param = np.linspace(0,1000,10001)
coef_mat = np.zeros((10001,5))

for i in range(10001):
    EN = ElasticNet(alpha=reg_param[i], l1_ratio=0.5)
    EN.fit(X,y)
    coef_mat[i,:] = EN.coef_
plt.plot(reg_param,coef_mat[:,0],label='x_1')
plt.plot(reg_param,coef_mat[:,1],label='x_2')
plt.plot(reg_param,coef_mat[:,2],label='x_3')
plt.plot(reg_param,coef_mat[:,3],label='x_4')
plt.plot(reg_param,coef_mat[:,4],label='x_5')
plt.xlabel('lambda')
plt.ylabel('weights')
plt.title('Elastic net')
plt.legend(loc='upper right')

for i in range(10001):
    LASSO = Lasso(alpha=reg_param[i])
    LASSO.fit(X,y)
    coef_mat[i,:] = LASSO.coef_
plt.plot(reg_param,coef_mat[:,0],label='x_1')
plt.plot(reg_param,coef_mat[:,1],label='x_2')
plt.plot(reg_param,coef_mat[:,2],label='x_3')
plt.plot(reg_param,coef_mat[:,3],label='x_4')
plt.plot(reg_param,coef_mat[:,4],label='x_5')
plt.xlabel('lambda')
plt.ylabel('weights')
plt.title('Lasso')
plt.legend(loc='upper right')
    
    
    
    
    
    
    
    
    
    
